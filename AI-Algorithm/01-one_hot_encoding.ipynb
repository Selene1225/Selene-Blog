{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a8f856",
   "metadata": {},
   "source": [
    "# 独热编码 (One-Hot Encoding)\n",
    "独热编码 (One-Hot Encoding) 看起来是最基础的数据预处理手段，但我们不能只把它看作简单的 get_dummies。\n",
    "\n",
    "我们要从 线性代数空间 和 计算图 (Computational Graph) 的角度去理解它：它是连接“离散符号世界”与“连续向量世界”的桥梁，也是所有 Embedding 技术的数学源头。\n",
    "\n",
    "## 1 概念与数学直觉\n",
    "\n",
    "**为什么不能直接用数字？**\n",
    "假设我们有三种类别： [猫, 狗, 鸟]。\n",
    "\n",
    "如果使用 Label Encoding 编码为 [1, 2, 3]：模型会认为\n",
    "\n",
    "1. $3 > 2 > 1$（鸟比狗“大”？）\n",
    "2. $\\text{Distance}(猫, 鸟) = 2$，而 $\\text{Distance}(猫, 狗) = 1$\n",
    "\n",
    "独热编码的几何意义我们将每个类别映射到一个 $N$ 维欧几里得空间的基向量 (Basis Vector) 上。\n",
    "若类别数为 $K$，对于第 $i$ 个类别，其向量 $e_i$ 为：\n",
    "$$e_i = [0, \\dots, 1, \\dots, 0] \\in \\mathbb{R}^K$$\n",
    "\n",
    "其中仅第 $i$ 位为 1\n",
    "\n",
    "核心性质：正交性 (Orthogonality) & 稀疏性 (Sparsity)\n",
    "这是独热编码最重要的数学性质：\n",
    "$$e_i^T \\cdot e_j = 0 \\quad (\\text{if } i \\neq j)$$\n",
    "$$||e_i - e_j||_2 = \\sqrt{2} \\quad (\\forall i \\neq j)$$\n",
    "\n",
    "- 在独热空间中，所有类别之间的距离都是相等的（$\\sqrt{2}$），且彼此正交（无关）。这消除了序数偏见。\n",
    "- 语义鸿沟。因为点积为 0，模型无法通过 $x_i \\cdot x_j$ 学到“猫”和“狗”比“猫”和“桌子”更相似。\n",
    "- 稀疏矩阵-种类多，则矩阵为 $N*N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb20dc",
   "metadata": {},
   "source": [
    "## 2 深入实践\n",
    "\n",
    "### 2.1 Phase 1: Numpy 手写底层（理解内存与索引）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22281fe8-6c05-4a7c-8b0f-7644c2bade25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: [1 4 0]\n",
      "one-hot output:\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "dot product of sample 0 and 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class OneHotEncoder:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "    def forward(self, indices):\n",
    "        batch_size = indices.shape[0]\n",
    "        # 创建 batch_size x num_classes 的零矩阵\n",
    "        one_hot = np.zeros((batch_size, self.num_classes), dtype=np.float32)\n",
    "        # [0,1],[1,4],[2,0] 位置赋值为1（花式索引：按轴分别指定索引）\n",
    "        one_hot[np.arange(batch_size), indices] = 1.0\n",
    "        return one_hot\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return None\n",
    "\n",
    "# 结果分类个数\n",
    "encoder = OneHotEncoder(num_classes=5)\n",
    "# 输入样本类别，0号样本是类1，1号样本是类4，2号样本是类0\n",
    "indices = np.array([1,4,0], dtype=np.int32)\n",
    "output = encoder.forward(indices)\n",
    "\n",
    "print(f\"indices: {indices}\")\n",
    "print(f\"one-hot output:\\n{output}\")\n",
    "print(f\"dot product of sample 0 and 1: {np.dot(output[0], output[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd983b3",
   "metadata": {},
   "source": [
    "### 2.2 Phase 2: PyTorch 中One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4dc453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu, One-hot with label smoothing:\n",
      "tensor([[0.9250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.9250, 0.0250]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def industrial_one_hot(indices, num_classes, lable_smoothing=0.0):\n",
    "    # PyTorch 的 one_hot 函数\n",
    "    one_hot = F.one_hot(indices, num_classes=num_classes).float()\n",
    "    if lable_smoothing > 0:\n",
    "        one_hot = one_hot * (1 - lable_smoothing) + lable_smoothing / num_classes\n",
    "    return one_hot\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "y_true = torch.tensor([0, 2], device=device)\n",
    "num_classes = 4\n",
    "y_encoded = industrial_one_hot(y_true, num_classes, lable_smoothing=0.1)\n",
    "print(f\"Device: {y_encoded.device}, One-hot with label smoothing:\\n{y_encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24768b99",
   "metadata": {},
   "source": [
    "### 2.3 Phase 3: 独热编码的陷阱\n",
    "1. 维度问题\n",
    "- 如果词表有5w个词，One-Hot向量长度就是5w\n",
    "- 一个Batch 64个句子，每个句子100个词。Tensor 大小: $64 \\times 100 \\times 50000 \\times 4 \\text{bytes} \\approx 1.28 \\text{GB}$\n",
    "显存瞬间打爆，改用 nn.Embedding （查表法）\n",
    "2. 类索引越界\n",
    "- 如果num_classes=10,但输入错误比如 10或-1\n",
    "- F.one_hot会报错或扩充为度，导致后面的linear层形状不匹配崩溃\n",
    "输入前 assert indices.max() < num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d211d71",
   "metadata": {},
   "source": [
    "## 3 落地架构\n",
    "真实的推荐系统或者LLM中，One-Hot仅仅是逻辑概念，物理实现通常被优化掉。\n",
    "核心：One-Hot + Linear = Embedding\n",
    "\n",
    "这是你需要刻在脑子里的公式。假设 $x$ 是 shape 为 $(1, V)$ 的 One-Hot 向量，$W$ 是 shape 为 $(V, D)$ 的权重矩阵。\n",
    "\n",
    "$$x \\cdot W = [0, \\dots, 1_i, \\dots, 0] \\cdot \\begin{bmatrix} w_{0} \\\\ \\vdots \\\\ w_{i} \\\\ \\vdots \\\\ w_{V-1} \\end{bmatrix} = w_{i}$$\n",
    "\n",
    "矩阵乘法等价于查表(Look-up):\n",
    "1. 类别少：星期几/性别 -> 直接用 One-Hot \n",
    "2. 类别多：UserID/WordID -> nn.Embedding(PyTorch) 或 tf.gather(TF)，底层只进行内存寻址拷贝，不进行矩阵乘法\n",
    "\n",
    "### 3.1 nn.Embedding 解决 One-Hot 问题\n",
    "首先一次内存寻址，复杂度为 $O(1)$ ，矩阵乘法需要 $V*D$ 次浮点运算，浪费算力去乘一堆0。\n",
    "内存上，将稀疏向量压缩为稠密向量，只存储有效信息。\n",
    "语义空间更合理，One-Hot任意两个词垂直，Embedding是可学习参数，训练后“猫”和“狗”向量在空间中会靠的很近，因为他们在类似的上下文中出现。\n",
    "连续可导，Embedding可以计算梯度并更新。\n",
    "\n",
    "下面手写一个支持反向传播的 Embedding 层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cfc05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward output:\n",
      " [[[ 0.01361072 -0.33824542  1.3603303 ]\n",
      "  [-0.5960619   0.41967472 -0.42378655]]\n",
      "\n",
      " [[ 0.01361072 -0.33824542  1.3603303 ]\n",
      "  [-1.1084248   0.24869247 -1.947749  ]]]\n",
      "gradient w.r.t. embedding weights:\n",
      " [[0. 0. 0.]\n",
      " [2. 2. 2.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class MyEmbedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # 随机初始化嵌入矩阵\n",
    "        self.weight = np.random.randn(num_embeddings, embedding_dim).astype(np.float32)\n",
    "        self.last_indices = None\n",
    "\n",
    "    def forward(self, indices):\n",
    "        self.last_indices = indices\n",
    "        # 使用花式索引获取嵌入向量\n",
    "        embedded = self.weight[indices]\n",
    "        return embedded\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_weight = np.zeros_like(self.weight)\n",
    "        flat_indices = self.last_indices.reshape(-1)\n",
    "        flat_grad = grad_output.reshape(-1, self.embedding_dim)\n",
    "        np.add.at(grad_weight, flat_indices, flat_grad)\n",
    "        return grad_weight\n",
    "\n",
    "# 创建嵌入层，词汇表大小10，嵌入维度3\n",
    "embedding = MyEmbedding(num_embeddings=5, embedding_dim=3)\n",
    "# 输入样本\n",
    "input_indices = np.array([[1, 2], [1, 4]])\n",
    "output = embedding.forward(input_indices)\n",
    "print(\"forward output:\\n\", output)\n",
    "\n",
    "fake_grad = np.ones_like(output)\n",
    "dw = embedding.backward(fake_grad)\n",
    "print(\"gradient w.r.t. embedding weights:\\n\", dw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079bdbb6",
   "metadata": {},
   "source": [
    "### 3.2 PyTorch\n",
    "PyTorch 中，nn.Embedding 封装了上面所有逻辑，并加上了CUDA优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. 定义\n",
    "embedding_layer = nn.Embedding(num_embeddings=10000, embedding_dim=256, padding_idx=0)\n",
    "# 2. 输入必须是 LongTensor\n",
    "input_indices = torch.tensor([[1, 2, 0], [4, 0, 5]], dtype=torch.long)\n",
    "# 3. 前向传播\n",
    "output = embedding_layer(input_indices)\n",
    "print(\"Embedding output shape:\", output.shape) # 应该是 (2, 3, 256)\n",
    "# 4. 反向传播\n",
    "output.sum().backward()\n",
    "print(\"Gradient w.r.t. embedding weights shape:\", embedding_layer.weight.grad.shape) # 应该是 (10000, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e64ab4",
   "metadata": {},
   "source": [
    "## 面试官视角 (Interviewer's Eye)\n",
    "Killer Question:\n",
    "> “既然 One-Hot 编码会导致正交，使得向量之间没有语义相似度（点积为0），为什么在 Transformer 或 BERT 的输入层，我们还是先要把 Token 变成 One-Hot（或者说索引），再过 Embedding 层？为什么不直接输入某种预训练好的语义向量？”\n",
    "\n",
    "S 级回答模板\n",
    "> 1. 澄清概念： 我们实际上输入的是 Token ID（索引），数学上等价于 One-Hot 乘以 Embedding 矩阵。\n",
    "> 2. 端到端学习 (End-to-End): 虽然 Word2Vec 等预训练向量有语义，但在特定任务（如法律文本分析 vs 社交媒体吐槽）中，词的语义是会漂移的。让模型通过 One-Hot 索引去从头学习（或微调） Embedding 矩阵 $W$，能让学到的向量空间最适配当前任务（Task-Specific）。\n",
    "> 3. 基底的完备性： One-Hot 是一组完备的正交基。它提供了“区分所有词”的最纯粹信息。如果一开始就输入 Dense Vector，可能会引入先验偏差（Bias），限制了模型在高维空间中重新组织语义的能力。\n",
    "\n",
    "## 下一步 (Next Step)\n",
    "One-Hot 只是数据进入神经网络的第一步。为了处理高维稀疏数据（如文本），我们必须从 One-Hot 进化到 Distributed Representation (分布式表示)。建议下一步：A. Embedding Layer (嵌入层) - 手写查表操作，理解它如何把 One-Hot 压缩成 Dense Vector。B. Softmax & CrossEntropy - One-Hot 在损失函数中是如何作为 Label 存在的？（非常经典的梯度推导）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnai1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
